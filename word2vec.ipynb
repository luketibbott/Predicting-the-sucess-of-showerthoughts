{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import redditutils as ru\n",
    "import word2vecReader as wvr\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include below until https://github.com/scikit-optimize/scikit-optimize/issues/718 is resolved\n",
    "class BayesSearchCV(BayesSearchCV):\n",
    "    def _run_search(self, x): raise BaseException('Use newer skopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_shower.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = df['score'].apply(lambda x: ru.make_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('shower_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 2325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[:100000]\n",
    "y_train_short = y_train[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shorter = X_train[:5000]\n",
    "y_train_shorter = y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_shorter = X_test[:2500]\n",
    "y_test_shorter = y_test[:2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./word2vec_twitter_model.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(words, model, num_features):\n",
    "    features = np.zeros(num_features)\n",
    "    \n",
    "    model_vocab = set(model.index2word)\n",
    "    \n",
    "    num_words = 0\n",
    "    \n",
    "    # Loop over words in documents. If the word is in model's vocabulary,\n",
    "    # generate its feature vector\n",
    "    for w in words:\n",
    "        if w in model_vocab:\n",
    "            num_words += 1\n",
    "            features = np.add(features, model[w])\n",
    "            \n",
    "    # Normalize the feature vector\n",
    "    features = np.divide(features, num_words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vecs(docs, model, num_features):\n",
    "    # Get the average feature vector for each showerthought based on the words it's comprised of\n",
    "    counter = 0\n",
    "    \n",
    "    doc_vector = np.zeros((len(docs), num_features))\n",
    "    \n",
    "    for d in docs:\n",
    "        if counter%100 == 0:\n",
    "            print(f'Finished document number {counter}')\n",
    "            \n",
    "        # Add this document's feature vector to doc_vector\n",
    "        doc_vector[counter] = make_features(d, model, num_features)\n",
    "            \n",
    "        counter += 1\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 100\n",
      "Finished document number 200\n",
      "Finished document number 300\n",
      "Finished document number 400\n",
      "Finished document number 500\n",
      "Finished document number 600\n",
      "Finished document number 700\n",
      "Finished document number 800\n",
      "Finished document number 900\n",
      "Finished document number 1000\n",
      "Finished document number 1100\n",
      "Finished document number 1200\n",
      "Finished document number 1300\n",
      "Finished document number 1400\n",
      "Finished document number 1500\n",
      "Finished document number 1600\n",
      "Finished document number 1700\n",
      "Finished document number 1800\n",
      "Finished document number 1900\n",
      "Finished document number 2000\n",
      "Finished document number 2100\n",
      "Finished document number 2200\n",
      "Finished document number 2300\n",
      "Finished document number 2400\n"
     ]
    }
   ],
   "source": [
    "features_test = document_vecs(X_test_shorter, model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 100\n",
      "Finished document number 200\n",
      "Finished document number 300\n",
      "Finished document number 400\n",
      "Finished document number 500\n",
      "Finished document number 600\n",
      "Finished document number 700\n",
      "Finished document number 800\n",
      "Finished document number 900\n"
     ]
    }
   ],
   "source": [
    "features_train = document_vecs(X_train_shorter[:1000], model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of redditutils failed: Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 368, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/anaconda3/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/anaconda3/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/luketibbott/Documents/Metis/project_fletcher/redditutils.py\", line 10, in <module>\n",
      "    from imblearn import SMOTE as sm\n",
      "ImportError: cannot import name 'SMOTE'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_train = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_indices(features, response):\n",
    "    # Remove document-response pairs that failed to be embedded in vector space\n",
    "    bad_indices = list(np.unique(np.where(np.isnan(features, axis=0)[0])))\n",
    "    \n",
    "    features = np.delete(features, bad_indices, axis=0)\n",
    "    response = np.delete(np.array(response), bad_indices)\n",
    "    \n",
    "    return features, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throw it in to a Random Forest\n",
    "\n",
    "Cluster if this doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 400)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57408518 0.56902761 0.57537538]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "score = cross_val_score(rfc, features, response, cv=3)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely some improvement with word2vec! ~.56 ROC AUC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(features, open('1k_features.pkl', 'wb'))\n",
    "pickle.dump(response, open('1k_response.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class BayesSearchCV with abstract methods _run_search",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-288-a68925f83fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                  'Kmeans__n_clusters': [i for i in range(1, 20)]}\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class BayesSearchCV with abstract methods _run_search"
     ]
    }
   ],
   "source": [
    "km = KMeans()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "pipe = make_pipeline(km, rfc)\n",
    "\n",
    "tuning_params = {'randomforestclassifier__n_estimators': [50, 100, 150, 200],\n",
    "                 'Kmeans__n_clusters': [i for i in range(1, 20)]}\n",
    "\n",
    "bs = BayesSearchCV(pipe, tuning_params, cv=3, scoring='roc_auc')\n",
    "\n",
    "bs.fit(features, response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

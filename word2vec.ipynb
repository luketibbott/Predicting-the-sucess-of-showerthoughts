{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import redditutils as ru\n",
    "import word2vecReader as wvr\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include below until https://github.com/scikit-optimize/scikit-optimize/issues/718 is resolved\n",
    "class BayesSearchCV(BayesSearchCV):\n",
    "    def _run_search(self, x): raise BaseException('Use newer skopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_shower.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = df['score'].apply(lambda x: ru.make_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('shower_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 2325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[:100000]\n",
    "y_train_short = y_train[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_short = X_test[:100000]\n",
    "y_test_short = y_test[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shorter = X_train[:5000]\n",
    "y_train_shorter = y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_shorter = X_test[:2500]\n",
    "y_test_shorter = y_test[:2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./word2vec_twitter_model.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(words, model, num_features):\n",
    "    features = np.zeros(num_features)\n",
    "    \n",
    "    model_vocab = set(model.index2word)\n",
    "    \n",
    "    num_words = 0\n",
    "    \n",
    "    # Loop over words in documents. If the word is in model's vocabulary,\n",
    "    # generate its feature vector\n",
    "    for w in words:\n",
    "        if w in model_vocab:\n",
    "            num_words += 1\n",
    "            features = np.add(features, model[w])\n",
    "            \n",
    "    # Normalize the feature vector\n",
    "    features = np.divide(features, num_words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vecs(docs, model, num_features):\n",
    "    # Get the average feature vector for each showerthought based on the words it's comprised of\n",
    "    counter = 0\n",
    "    \n",
    "    doc_vector = np.zeros((len(docs), num_features))\n",
    "    \n",
    "    for d in docs:\n",
    "        if counter%100 == 0:\n",
    "            print(f'Finished document number {counter}')\n",
    "            \n",
    "        # Add this document's feature vector to doc_vector\n",
    "        doc_vector[counter] = make_features(d, model, num_features)\n",
    "            \n",
    "        counter += 1\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 100\n",
      "Finished document number 200\n",
      "Finished document number 300\n",
      "Finished document number 400\n",
      "Finished document number 500\n",
      "Finished document number 600\n",
      "Finished document number 700\n",
      "Finished document number 800\n",
      "Finished document number 900\n",
      "Finished document number 1000\n",
      "Finished document number 1100\n",
      "Finished document number 1200\n",
      "Finished document number 1300\n",
      "Finished document number 1400\n",
      "Finished document number 1500\n",
      "Finished document number 1600\n",
      "Finished document number 1700\n",
      "Finished document number 1800\n",
      "Finished document number 1900\n",
      "Finished document number 2000\n",
      "Finished document number 2100\n",
      "Finished document number 2200\n",
      "Finished document number 2300\n",
      "Finished document number 2400\n"
     ]
    }
   ],
   "source": [
    "features_test = document_vecs(X_test_shorter, model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 100\n",
      "Finished document number 200\n",
      "Finished document number 300\n",
      "Finished document number 400\n",
      "Finished document number 500\n",
      "Finished document number 600\n",
      "Finished document number 700\n",
      "Finished document number 800\n",
      "Finished document number 900\n"
     ]
    }
   ],
   "source": [
    "features_train = document_vecs(X_train_shorter[:1000], model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of redditutils failed: Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 368, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/anaconda3/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/anaconda3/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/luketibbott/Documents/Metis/project_fletcher/redditutils.py\", line 10, in <module>\n",
      "    from imblearn import SMOTE as sm\n",
      "ImportError: cannot import name 'SMOTE'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_train = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_indices(features, response):\n",
    "    # Remove document-response pairs that failed to be embedded in vector space\n",
    "    bad_indices = list(np.unique(np.where(np.isnan(features, axis=0)[0])))\n",
    "    \n",
    "    features = np.delete(features, bad_indices, axis=0)\n",
    "    response = np.delete(np.array(response), bad_indices)\n",
    "    \n",
    "    return features, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 100\n",
      "Finished document number 200\n",
      "Finished document number 300\n",
      "Finished document number 400\n",
      "Finished document number 500\n",
      "Finished document number 600\n",
      "Finished document number 700\n",
      "Finished document number 800\n",
      "Finished document number 900\n",
      "Finished document number 1000\n",
      "Finished document number 1100\n",
      "Finished document number 1200\n",
      "Finished document number 1300\n",
      "Finished document number 1400\n",
      "Finished document number 1500\n",
      "Finished document number 1600\n",
      "Finished document number 1700\n",
      "Finished document number 1800\n",
      "Finished document number 1900\n",
      "Finished document number 2000\n",
      "Finished document number 2100\n",
      "Finished document number 2200\n",
      "Finished document number 2300\n",
      "Finished document number 2400\n",
      "Finished document number 2500\n",
      "Finished document number 2600\n",
      "Finished document number 2700\n",
      "Finished document number 2800\n",
      "Finished document number 2900\n",
      "Finished document number 3000\n",
      "Finished document number 3100\n",
      "Finished document number 3200\n",
      "Finished document number 3300\n",
      "Finished document number 3400\n",
      "Finished document number 3500\n",
      "Finished document number 3600\n",
      "Finished document number 3700\n",
      "Finished document number 3800\n",
      "Finished document number 3900\n",
      "Finished document number 4000\n",
      "Finished document number 4100\n",
      "Finished document number 4200\n",
      "Finished document number 4300\n",
      "Finished document number 4400\n",
      "Finished document number 4500\n",
      "Finished document number 4600\n",
      "Finished document number 4700\n",
      "Finished document number 4800\n",
      "Finished document number 4900\n",
      "Finished document number 5000\n",
      "Finished document number 5100\n",
      "Finished document number 5200\n",
      "Finished document number 5300\n",
      "Finished document number 5400\n",
      "Finished document number 5500\n",
      "Finished document number 5600\n",
      "Finished document number 5700\n",
      "Finished document number 5800\n",
      "Finished document number 5900\n",
      "Finished document number 6000\n",
      "Finished document number 6100\n",
      "Finished document number 6200\n",
      "Finished document number 6300\n",
      "Finished document number 6400\n",
      "Finished document number 6500\n",
      "Finished document number 6600\n",
      "Finished document number 6700\n",
      "Finished document number 6800\n",
      "Finished document number 6900\n",
      "Finished document number 7000\n",
      "Finished document number 7100\n",
      "Finished document number 7200\n",
      "Finished document number 7300\n",
      "Finished document number 7400\n",
      "Finished document number 7500\n",
      "Finished document number 7600\n",
      "Finished document number 7700\n",
      "Finished document number 7800\n",
      "Finished document number 7900\n",
      "Finished document number 8000\n",
      "Finished document number 8100\n",
      "Finished document number 8200\n",
      "Finished document number 8300\n",
      "Finished document number 8400\n",
      "Finished document number 8500\n",
      "Finished document number 8600\n",
      "Finished document number 8700\n",
      "Finished document number 8800\n",
      "Finished document number 8900\n",
      "Finished document number 9000\n",
      "Finished document number 9100\n",
      "Finished document number 9200\n",
      "Finished document number 9300\n",
      "Finished document number 9400\n",
      "Finished document number 9500\n",
      "Finished document number 9600\n",
      "Finished document number 9700\n",
      "Finished document number 9800\n",
      "Finished document number 9900\n",
      "Finished document number 10000\n",
      "Finished document number 10100\n",
      "Finished document number 10200\n",
      "Finished document number 10300\n",
      "Finished document number 10400\n",
      "Finished document number 10500\n",
      "Finished document number 10600\n",
      "Finished document number 10700\n",
      "Finished document number 10800\n",
      "Finished document number 10900\n",
      "Finished document number 11000\n",
      "Finished document number 11100\n",
      "Finished document number 11200\n",
      "Finished document number 11300\n",
      "Finished document number 11400\n",
      "Finished document number 11500\n",
      "Finished document number 11600\n",
      "Finished document number 11700\n",
      "Finished document number 11800\n",
      "Finished document number 11900\n",
      "Finished document number 12000\n",
      "Finished document number 12100\n",
      "Finished document number 12200\n",
      "Finished document number 12300\n",
      "Finished document number 12400\n",
      "Finished document number 12500\n",
      "Finished document number 12600\n",
      "Finished document number 12700\n",
      "Finished document number 12800\n",
      "Finished document number 12900\n",
      "Finished document number 13000\n",
      "Finished document number 13100\n",
      "Finished document number 13200\n",
      "Finished document number 13300\n",
      "Finished document number 13400\n",
      "Finished document number 13500\n",
      "Finished document number 13600\n",
      "Finished document number 13700\n",
      "Finished document number 13800\n",
      "Finished document number 13900\n",
      "Finished document number 14000\n",
      "Finished document number 14100\n",
      "Finished document number 14200\n",
      "Finished document number 14300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 14400\n",
      "Finished document number 14500\n",
      "Finished document number 14600\n",
      "Finished document number 14700\n",
      "Finished document number 14800\n",
      "Finished document number 14900\n",
      "Finished document number 15000\n",
      "Finished document number 15100\n",
      "Finished document number 15200\n",
      "Finished document number 15300\n",
      "Finished document number 15400\n",
      "Finished document number 15500\n",
      "Finished document number 15600\n",
      "Finished document number 15700\n",
      "Finished document number 15800\n",
      "Finished document number 15900\n",
      "Finished document number 16000\n",
      "Finished document number 16100\n",
      "Finished document number 16200\n",
      "Finished document number 16300\n",
      "Finished document number 16400\n",
      "Finished document number 16500\n",
      "Finished document number 16600\n",
      "Finished document number 16700\n",
      "Finished document number 16800\n",
      "Finished document number 16900\n",
      "Finished document number 17000\n",
      "Finished document number 17100\n",
      "Finished document number 17200\n",
      "Finished document number 17300\n",
      "Finished document number 17400\n",
      "Finished document number 17500\n",
      "Finished document number 17600\n",
      "Finished document number 17700\n",
      "Finished document number 17800\n",
      "Finished document number 17900\n",
      "Finished document number 18000\n",
      "Finished document number 18100\n",
      "Finished document number 18200\n",
      "Finished document number 18300\n",
      "Finished document number 18400\n",
      "Finished document number 18500\n",
      "Finished document number 18600\n",
      "Finished document number 18700\n",
      "Finished document number 18800\n",
      "Finished document number 18900\n",
      "Finished document number 19000\n",
      "Finished document number 19100\n",
      "Finished document number 19200\n",
      "Finished document number 19300\n",
      "Finished document number 19400\n",
      "Finished document number 19500\n",
      "Finished document number 19600\n",
      "Finished document number 19700\n",
      "Finished document number 19800\n",
      "Finished document number 19900\n",
      "Finished document number 20000\n",
      "Finished document number 20100\n",
      "Finished document number 20200\n",
      "Finished document number 20300\n",
      "Finished document number 20400\n",
      "Finished document number 20500\n",
      "Finished document number 20600\n",
      "Finished document number 20700\n",
      "Finished document number 20800\n",
      "Finished document number 20900\n",
      "Finished document number 21000\n",
      "Finished document number 21100\n",
      "Finished document number 21200\n",
      "Finished document number 21300\n",
      "Finished document number 21400\n",
      "Finished document number 21500\n",
      "Finished document number 21600\n",
      "Finished document number 21700\n",
      "Finished document number 21800\n",
      "Finished document number 21900\n",
      "Finished document number 22000\n",
      "Finished document number 22100\n",
      "Finished document number 22200\n",
      "Finished document number 22300\n",
      "Finished document number 22400\n",
      "Finished document number 22500\n",
      "Finished document number 22600\n",
      "Finished document number 22700\n",
      "Finished document number 22800\n",
      "Finished document number 22900\n",
      "Finished document number 23000\n",
      "Finished document number 23100\n",
      "Finished document number 23200\n",
      "Finished document number 23300\n"
     ]
    }
   ],
   "source": [
    "one_hunnid_train = document_vecs(X_train_short, model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(one_hunnid_train, open('100k_train.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throw it in to a Random Forest\n",
    "\n",
    "Cluster if this doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 400)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5764847  0.56722689 0.57297297]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "score = cross_val_score(rfc, features, response, cv=3)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely some improvement with word2vec! ~.56 ROC AUC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(features, open('1k_features.pkl', 'wb'))\n",
    "pickle.dump(response, open('1k_response.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5159584307632196"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "pipe = make_pipeline(km, rfc)\n",
    "\n",
    "tuning_params = {'randomforestclassifier__n_estimators': [50, 100, 150, 200],\n",
    "                 'kmeans__n_clusters': [i for i in range(1, 20)]}\n",
    "\n",
    "gs = RandomizedSearchCV(pipe, tuning_params, cv=3, scoring='roc_auc')\n",
    "\n",
    "gs.fit(features, response)\n",
    "\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of DBSCAN(algorithm='auto', eps=0.001, leaf_size=30, metric='cosine',\n",
       "    metric_params=None, min_samples=10, n_jobs=None, p=None)>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = DBSCAN(eps=0.001, min_samples=10, metric='cosine').fit(features)\n",
    "\n",
    "\n",
    "\n",
    "clusters.get_params\n",
    "#rfc = RandomForestClassifier()\n",
    "\n",
    "#scores = np.mean(cross_val_score(pipe, clusters.labels_, response, cv=3))\n",
    "\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.labels_.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import redditutils as ru\n",
    "import word2vecReader as wvr\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "import pickle\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100k_response.pkl             reddit-ETL.ipynb\r\n",
      "100k_train.pkl                redditutils.py\r\n",
      "1k_features.pkl               results.csv\r\n",
      "1k_response.pkl               \u001b[34mseinfeld-chronicles\u001b[m\u001b[m\r\n",
      "25k_test.pkl                  short_test_matrix.csv.mtx\r\n",
      "Model_tuning.ipynb            short_train_matrix.csv.mtx\r\n",
      "Project Fletcher Proposal.pdf shower_clean.csv\r\n",
      "README.md                     showerthoughts-clean.ipynb\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                   showerthoughts.csv\r\n",
      "bayes_search.pkl              test.csv\r\n",
      "classification.ipynb          test_csv.csv.\r\n",
      "cleaned_shower.csv            tfidf.ipynb\r\n",
      "cleaning.csv                  tokenized.csv\r\n",
      "count-vec-models.ipynb        train.csv\r\n",
      "darkweb-EDA.ipynb             vectorized_df\r\n",
      "first_5k_response.pkl         vectorized_df.csv\r\n",
      "first_5k_words.pkl            word2vec.ipynb\r\n",
      "fitted_cv.pkl                 word2vecReader.py\r\n",
      "fitted_lda_short.pkl          word2vecReaderUtils.py\r\n",
      "\u001b[34mflask\u001b[m\u001b[m                         \u001b[34mword2vec_twitter_model\u001b[m\u001b[m\r\n",
      "model-tuning.ipynb            word2vec_twitter_model.bin\r\n",
      "random_forest.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_shower.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = df['score'].apply(lambda x: ru.make_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 2325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[:100000]\n",
    "y_train_short = y_train[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English words\n",
    "words_corpus = set(words.words())\n",
    "# Stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "# English words minus stop words\n",
    "acceptable_words = words_corpus - stop\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "stem = SnowballStemmer('english')\n",
    "\n",
    "def english_corpus(doc, stemmer=stem):\n",
    "    return [stemmer.stem(w) for w in analyzer(doc) if w in acceptable_words]\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', \n",
    "                     min_df = 2,\n",
    "                     max_df = .15, \n",
    "                     tokenizer=english_corpus,\n",
    "                     strip_accents='unicode',\n",
    "                     encoding='utf-8', \n",
    "                     ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_train_short_dtm = cv.fit_transform(X_train_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "rfc_scores = cross_val_score(rfc, X_train_short_dtm, y_train_short, cv=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train_short_dtm, y_train_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc, open('random_forest.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English words\n",
    "words_corpus = set(words.words())\n",
    "# Stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "# English words minus stop words\n",
    "acceptable_words = words_corpus - stop\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "stem = SnowballStemmer('english')\n",
    "\n",
    "def english_corpus(doc, stemmer=stem):\n",
    "    return [stemmer.stem(w) for w in analyzer(doc) if w in acceptable_words]\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = \"english\",\n",
    "                        strip_accents = 'ascii',\n",
    "                        max_df = .10,\n",
    "                        min_df = 3, \n",
    "                        tokenizer = english_corpus,\n",
    "                        ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_train_short_tf = tfidf.fit_transform(X_train_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "rfc_scores = cross_val_score(rfc, X_train_short_tf, y_train_short, cv=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "nb_scores = cross_val_score(nb, X_train_short_tf.toarray(), y_train_short, cv=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52520644, 0.52275651, 0.51909955])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59729186, 0.5928255 , 0.58642293])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try clustering TF-IDF with KMeans / DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 14)\n",
    "\n",
    "km.fit(X_train_short_tf)\n",
    "\n",
    "features = pd.get_dummies(km.labels_)\n",
    "\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rfc_scores = cross_val_score(rfc, features, y_train_short, cv=3, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53619818, 0.53488184, 0.53566778])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "    metric_params=None, min_samples=5, n_jobs=None, p=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = DBSCAN()\n",
    "\n",
    "db.fit(X_train_short_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,\n",
       "        12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,\n",
       "        25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,\n",
       "        38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,\n",
       "        51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
       "        64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n",
       "        77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
       "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "       103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "       116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
       "       129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
       "       142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "       155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "       168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180,\n",
       "       181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193,\n",
       "       194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
       "       207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
       "       220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "       233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245,\n",
       "       246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258,\n",
       "       259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271,\n",
       "       272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284,\n",
       "       285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
       "       298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
       "       311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "       324, 325, 326, 327, 328])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfcdb = RandomForestClassifier()\n",
    "\n",
    "cross_val_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

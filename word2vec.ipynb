{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import redditutils as ru\n",
    "import word2vecReader as wvr\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_shower.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = df['score'].apply(lambda x: ru.make_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('shower_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 2325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[:100000]\n",
    "y_train_short = y_train[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shorter = X_train[:5000]\n",
    "y_train_shorter = y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_shorter = X_test[:2500]\n",
    "y_test_shorter = y_test[:2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./word2vec_twitter_model.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(words, model, num_features):\n",
    "    features = np.zeros(num_features)\n",
    "    \n",
    "    model_vocab = set(model.index2word)\n",
    "    \n",
    "    num_words = 0\n",
    "    \n",
    "    # Loop over words in documents. If the word is in model's vocabulary,\n",
    "    # generate its feature vector\n",
    "    for w in words:\n",
    "        if w in model_vocab:\n",
    "            num_words += 1\n",
    "            features = np.add(features, model[w])\n",
    "            \n",
    "    # Normalize the feature vector\n",
    "    features = np.divide(features, num_words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vecs(docs, model, num_features):\n",
    "    # Get the average feature vector for each showerthought based on the words it's comprised of\n",
    "    counter = 0\n",
    "    \n",
    "    doc_vector = np.zeros((len(docs), num_features))\n",
    "    \n",
    "    for d in docs:\n",
    "        if counter%100 == 0:\n",
    "            print(f'Finished document number {counter}')\n",
    "            \n",
    "        # Add this document's feature vector to doc_vector\n",
    "        doc_vector[counter] = make_features(d, model, num_features)\n",
    "            \n",
    "        counter += 1\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 100\n",
      "Finished document number 200\n",
      "Finished document number 300\n",
      "Finished document number 400\n",
      "Finished document number 500\n",
      "Finished document number 600\n",
      "Finished document number 700\n",
      "Finished document number 800\n",
      "Finished document number 900\n",
      "Finished document number 1000\n",
      "Finished document number 1100\n",
      "Finished document number 1200\n",
      "Finished document number 1300\n",
      "Finished document number 1400\n",
      "Finished document number 1500\n",
      "Finished document number 1600\n",
      "Finished document number 1700\n",
      "Finished document number 1800\n",
      "Finished document number 1900\n",
      "Finished document number 2000\n",
      "Finished document number 2100\n",
      "Finished document number 2200\n",
      "Finished document number 2300\n",
      "Finished document number 2400\n"
     ]
    }
   ],
   "source": [
    "features_test = document_vecs(X_test_shorter, model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 100\n",
      "Finished document number 200\n",
      "Finished document number 300\n",
      "Finished document number 400\n",
      "Finished document number 500\n",
      "Finished document number 600\n",
      "Finished document number 700\n",
      "Finished document number 800\n"
     ]
    }
   ],
   "source": [
    "features_train = document_vecs(X_train_shorter[:1000], model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train = y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_indices(features, response):\n",
    "    # Remove document-response pairs that failed to be embedded in vector space\n",
    "    bad_indices = list(np.unique(np.where(np.isnan(features, axis=0)[0])))\n",
    "    \n",
    "    features = np.delete(features, bad_indices, axis=0)\n",
    "    response = np.delete(np.array(response), bad_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throw it in to a Random Forest\n",
    "\n",
    "Cluster if this doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 400)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55728854 0.56422569 0.56816817]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "score = cross_val_score(rfc, features_train_dropna, y_train_shorter, cv=3)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely some improvement with word2vec! ~.56 ROC AUC!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "pipe = make_pipeline(km, rfc)\n",
    "\n",
    "tuning_params = {'randomforestclassifier__n_estimators': [50, 100, 150, 200],\n",
    "                 'Kmeans__n_clusters': [i for i in range(1, 20)]}\n",
    "\n",
    "bs = BayesSearchCV(pipe, tuning_params, cv=3, scoring='roc_auc')\n",
    "\n",
    "bs.fit(features, y_train_shorter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

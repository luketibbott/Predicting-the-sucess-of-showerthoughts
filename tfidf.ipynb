{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import redditutils as ru\n",
    "import pickle\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.io import mmwrite, mmread\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700000</th>\n",
       "      <td>livesNbox</td>\n",
       "      <td>I wonder if anyone sells dandelion seeds</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.494614e+09</td>\n",
       "      <td>66dags</td>\n",
       "      <td>2017-04-19 21:33:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700001</th>\n",
       "      <td>keepmovingon69</td>\n",
       "      <td>Before phone and telegram lines, you could pro...</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.463498e+09</td>\n",
       "      <td>4c0ksq</td>\n",
       "      <td>2016-03-26 09:01:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700002</th>\n",
       "      <td>vorpike</td>\n",
       "      <td>As an Asian idk why we stereotypically pronoun...</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.473036e+09</td>\n",
       "      <td>4x6gd8</td>\n",
       "      <td>2016-08-11 05:17:10</td>\n",
       "      <td>Some of us can't roll our tongues and pronounc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700003</th>\n",
       "      <td>CappyCrunch</td>\n",
       "      <td>When you're at someone else's house the loudes...</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.492418e+09</td>\n",
       "      <td>5zt2xp</td>\n",
       "      <td>2017-03-16 20:28:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700004</th>\n",
       "      <td>Fart_Kontrol</td>\n",
       "      <td>Why didn't Uncle Phillip buy the Fresh Prince'...</td>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.492411e+09</td>\n",
       "      <td>5zevhw</td>\n",
       "      <td>2017-03-14 20:56:38</td>\n",
       "      <td>I mean, Fresh P is in high school.  Seems like...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                              title  \\\n",
       "700000       livesNbox           I wonder if anyone sells dandelion seeds   \n",
       "700001  keepmovingon69  Before phone and telegram lines, you could pro...   \n",
       "700002         vorpike  As an Asian idk why we stereotypically pronoun...   \n",
       "700003     CappyCrunch  When you're at someone else's house the loudes...   \n",
       "700004    Fart_Kontrol  Why didn't Uncle Phillip buy the Fresh Prince'...   \n",
       "\n",
       "             subreddit  score  num_comments  retrieved_on      id  \\\n",
       "700000  Showerthoughts    3.0           2.0  1.494614e+09  66dags   \n",
       "700001  Showerthoughts    3.0           2.0  1.463498e+09  4c0ksq   \n",
       "700002  Showerthoughts    3.0           2.0  1.473036e+09  4x6gd8   \n",
       "700003  Showerthoughts    3.0           2.0  1.492418e+09  5zt2xp   \n",
       "700004  Showerthoughts    3.0           2.0  1.492411e+09  5zevhw   \n",
       "\n",
       "                created_utc  \\\n",
       "700000  2017-04-19 21:33:46   \n",
       "700001  2016-03-26 09:01:46   \n",
       "700002  2016-08-11 05:17:10   \n",
       "700003  2017-03-16 20:28:14   \n",
       "700004  2017-03-14 20:56:38   \n",
       "\n",
       "                                                 selftext  quality  \n",
       "700000                                                NaN      1.0  \n",
       "700001                                                NaN      1.0  \n",
       "700002  Some of us can't roll our tongues and pronounc...      1.0  \n",
       "700003                                                NaN      1.0  \n",
       "700004  I mean, Fresh P is in high school.  Seems like...      1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quality'] = train['quality'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = train['quality']\n",
    "feat = train['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat, resp, stratify = resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[:50000]\n",
    "y_train_short = y_train[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English words\n",
    "words_corpus = set(words.words())\n",
    "# Stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "# English words minus stop words\n",
    "acceptable_words = words_corpus - stop\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "stem = SnowballStemmer('english')\n",
    "\n",
    "def english_corpus(doc, stemmer=stem):\n",
    "    return [stemmer.stem(w) for w in analyzer(doc) if w in acceptable_words]\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = \"english\",\n",
    "                        strip_accents = 'ascii',\n",
    "                        max_df = .10,\n",
    "                        min_df = 3, \n",
    "                        tokenizer = english_corpus,\n",
    "                        ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Every time I explain something to my five-year-old, I have to explain it like he's five.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_short[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_tf = cv.fit_transform(X_train_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 4093,\n",
       "         5: 34425,\n",
       "         9: 3408,\n",
       "         6: 1374,\n",
       "         4: 1561,\n",
       "         2: 1826,\n",
       "         8: 1438,\n",
       "         1: 1791,\n",
       "         0: 83,\n",
       "         7: 1})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.10705556, 4.08615265, 4.07685019, ..., 7.48331477, 4.08688647,\n",
       "        4.12808344],\n",
       "       [2.66172366, 2.66872706, 2.67983135, ..., 6.92820323, 2.71916878,\n",
       "        2.68789106],\n",
       "       [3.59724819, 3.58961904, 3.58326185, ..., 7.34846923, 3.63057617,\n",
       "        3.30785346],\n",
       "       ...,\n",
       "       [3.62394338, 3.61071119, 3.60990877, ..., 7.34846923, 3.64948963,\n",
       "        3.63456834],\n",
       "       [2.85605707, 2.86107597, 2.87655971, ..., 7.        , 2.90991277,\n",
       "        2.89274106],\n",
       "       [2.03888744, 2.04509306, 1.46058321, ..., 6.55743852, 2.10938656,\n",
       "        2.09627272]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters = 10)\n",
    "\n",
    "km.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = km.labels_\n",
    "\n",
    "features = pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_res, response_res = ru.upsample(features, np.array(y_train_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51837419, 0.51947646, 0.52538815])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "cross_val_score(rfc, features, y_train_short, cv=3, scoring='roc_auc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

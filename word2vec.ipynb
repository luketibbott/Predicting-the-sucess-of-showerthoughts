{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import redditutils as ru\n",
    "import word2vecReader as wvr\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README                     word2vecReaderUtils.py\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                word2vec_twitter_model.bin\r\n",
      "word2vecReader.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls word2vec_twitter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_shower.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = df['score'].apply(lambda x: ru.make_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3927000364998175"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quality.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[:100000]\n",
    "y_train_short = y_train[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./word2vec_twitter_model.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(words, model, num_features):\n",
    "    features = np.zeros(num_features)\n",
    "    \n",
    "    model_vocab = set(model.index2word)\n",
    "    \n",
    "    num_words = 0\n",
    "    \n",
    "    # Loop over words in documents. If the word is in model's vocabulary,\n",
    "    # generate its feature vector\n",
    "    for w in words:\n",
    "        if w in model_vocab:\n",
    "            num_words += 1\n",
    "            features = np.add(features, model[w])\n",
    "            \n",
    "    # Normalize the feature vector\n",
    "    features = np.divide(features, num_words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vecs(docs, model, num_features):\n",
    "    # Get the average feature vector for each showerthought based on the words it's comprised of\n",
    "    counter = 0\n",
    "    \n",
    "    doc_vector = np.zeros((len(docs), num_features))\n",
    "    \n",
    "    for d in docs:\n",
    "        if counter%1000 == 0:\n",
    "            print(f'Finished document number {counter}')\n",
    "            \n",
    "            # Add this document's feature vector to doc_vector\n",
    "            doc_vector[counter] = make_features(d, model, num_features)\n",
    "            \n",
    "        counter += 1\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished document number 0\n",
      "Finished document number 1000\n",
      "Finished document number 2000\n",
      "Finished document number 3000\n",
      "Finished document number 4000\n",
      "Finished document number 5000\n",
      "Finished document number 6000\n",
      "Finished document number 7000\n",
      "Finished document number 8000\n",
      "Finished document number 9000\n",
      "Finished document number 10000\n",
      "Finished document number 11000\n",
      "Finished document number 12000\n",
      "Finished document number 13000\n",
      "Finished document number 14000\n",
      "Finished document number 15000\n",
      "Finished document number 16000\n",
      "Finished document number 17000\n",
      "Finished document number 18000\n",
      "Finished document number 19000\n",
      "Finished document number 20000\n",
      "Finished document number 21000\n",
      "Finished document number 22000\n",
      "Finished document number 23000\n",
      "Finished document number 24000\n",
      "Finished document number 25000\n",
      "Finished document number 26000\n",
      "Finished document number 27000\n",
      "Finished document number 28000\n",
      "Finished document number 29000\n",
      "Finished document number 30000\n",
      "Finished document number 31000\n",
      "Finished document number 32000\n",
      "Finished document number 33000\n",
      "Finished document number 34000\n",
      "Finished document number 35000\n",
      "Finished document number 36000\n",
      "Finished document number 37000\n",
      "Finished document number 38000\n",
      "Finished document number 39000\n",
      "Finished document number 40000\n",
      "Finished document number 41000\n",
      "Finished document number 42000\n",
      "Finished document number 43000\n",
      "Finished document number 44000\n",
      "Finished document number 45000\n",
      "Finished document number 46000\n",
      "Finished document number 47000\n",
      "Finished document number 48000\n",
      "Finished document number 49000\n",
      "Finished document number 50000\n",
      "Finished document number 51000\n",
      "Finished document number 52000\n",
      "Finished document number 53000\n",
      "Finished document number 54000\n",
      "Finished document number 55000\n",
      "Finished document number 56000\n",
      "Finished document number 57000\n",
      "Finished document number 58000\n",
      "Finished document number 59000\n",
      "Finished document number 60000\n",
      "Finished document number 61000\n",
      "Finished document number 62000\n",
      "Finished document number 63000\n",
      "Finished document number 64000\n",
      "Finished document number 65000\n",
      "Finished document number 66000\n",
      "Finished document number 67000\n",
      "Finished document number 68000\n",
      "Finished document number 69000\n",
      "Finished document number 70000\n",
      "Finished document number 71000\n",
      "Finished document number 72000\n",
      "Finished document number 73000\n",
      "Finished document number 74000\n",
      "Finished document number 75000\n",
      "Finished document number 76000\n",
      "Finished document number 77000\n",
      "Finished document number 78000\n",
      "Finished document number 79000\n",
      "Finished document number 80000\n",
      "Finished document number 81000\n",
      "Finished document number 82000\n",
      "Finished document number 83000\n",
      "Finished document number 84000\n",
      "Finished document number 85000\n",
      "Finished document number 86000\n",
      "Finished document number 87000\n",
      "Finished document number 88000\n",
      "Finished document number 89000\n",
      "Finished document number 90000\n",
      "Finished document number 91000\n",
      "Finished document number 92000\n",
      "Finished document number 93000\n",
      "Finished document number 94000\n",
      "Finished document number 95000\n",
      "Finished document number 96000\n",
      "Finished document number 97000\n",
      "Finished document number 98000\n",
      "Finished document number 99000\n"
     ]
    }
   ],
   "source": [
    "features = document_vecs(X_train_short, model, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
